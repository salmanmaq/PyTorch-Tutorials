{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Bag-of-Words Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ff0542b5990>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gusta': 1, 'en': 3, 'it': 7, 'not': 17, 'is': 16, 'good': 19, 'buena': 14, 'a': 18, 'Give': 6, 'Yo': 23, 'idea': 15, 'get': 20, 'creo': 10, 'si': 24, 'at': 22, 'cafeteria': 5, 'sea': 12, 'lost': 21, 'una': 13, 'to': 8, 'la': 4, 'me': 0, 'No': 9, 'on': 25, 'que': 11, 'comer': 2}\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# Map each word to a unique integer which represents it's index in the vocabulary\n",
    "word_to_idx = {} \n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "print(word_to_idx)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "NUM_LABELS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoWClassifier(nn.Module):\n",
    "    '''\n",
    "        The bag of words classifier model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        super(BoWClassifier, self).__init__()\n",
    "        \n",
    "        # Define the parameters\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "        \n",
    "    def forward(self, bow_vec):\n",
    "        return F.log_softmax(self.linear(bow_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, word_to_idx):\n",
    "    '''\n",
    "        Converts sentences to their bag of words vectors\n",
    "    '''\n",
    "    vec = torch.zeros(len(word_to_idx))\n",
    "    for word in sentence:\n",
    "        vec[word_to_idx[word]] +=1\n",
    "    return vec.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_target(label, label_to_idx):\n",
    "    return torch.LongTensor([label_to_idx[label]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "\n",
      "Columns 0 to 9 \n",
      "-0.0033  0.0331 -0.1752  0.0259  0.0291  0.0447 -0.1386  0.1791  0.0350 -0.0938\n",
      " 0.0058  0.1147  0.1744 -0.1844  0.0339  0.1503  0.1582  0.0160 -0.1422 -0.0204\n",
      "\n",
      "Columns 10 to 19 \n",
      " 0.0784 -0.1055 -0.1560  0.0131 -0.0337  0.1765  0.0763 -0.0027 -0.0337  0.0159\n",
      "-0.1415  0.1538  0.1206 -0.0480 -0.0401  0.0151 -0.1313  0.0597  0.1677 -0.0544\n",
      "\n",
      "Columns 20 to 25 \n",
      "-0.1765  0.1041  0.0141 -0.1783  0.0642 -0.1412\n",
      "-0.0597  0.0279  0.0984  0.0541  0.0886 -0.1466\n",
      "[torch.FloatTensor of size 2x26]\n",
      "\n",
      "Parameter containing:\n",
      " 0.1503\n",
      " 0.0746\n",
      "[torch.FloatTensor of size 2]\n",
      "\n",
      "Variable containing:\n",
      "-0.8343 -0.5695\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param)\n",
    "    \n",
    "sample = data[0]\n",
    "bow_vector = make_bow_vector(sample[0], word_to_idx)\n",
    "log_probs = model(autograd.Variable(bow_vector))\n",
    "print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_idx = {\"SPANISH\": 0, \"ENGLISH\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.8095 -0.5890\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-0.4670 -0.9858\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Forward pass on the test data once\n",
    "for instance, _ in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_idx))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.0784\n",
      "-0.1415\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print parameters corresponging to the word \"creo\"\n",
    "print(next(model.parameters())[:, word_to_idx[\"creo\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Element: 0, Loss: [ 0.00343033]\n",
      "Epoch: 0, Element: 1, Loss: [ 0.00446006]\n",
      "Epoch: 0, Element: 2, Loss: [ 0.00292907]\n",
      "Epoch: 0, Element: 3, Loss: [ 0.00142599]\n",
      "Epoch: 1, Element: 0, Loss: [ 0.003419]\n",
      "Epoch: 1, Element: 1, Loss: [ 0.00444515]\n",
      "Epoch: 1, Element: 2, Loss: [ 0.00291933]\n",
      "Epoch: 1, Element: 3, Loss: [ 0.00142122]\n",
      "Epoch: 2, Element: 0, Loss: [ 0.00340774]\n",
      "Epoch: 2, Element: 1, Loss: [ 0.00443035]\n",
      "Epoch: 2, Element: 2, Loss: [ 0.00290966]\n",
      "Epoch: 2, Element: 3, Loss: [ 0.00141649]\n",
      "Epoch: 3, Element: 0, Loss: [ 0.00339656]\n",
      "Epoch: 3, Element: 1, Loss: [ 0.00441564]\n",
      "Epoch: 3, Element: 2, Loss: [ 0.00290004]\n",
      "Epoch: 3, Element: 3, Loss: [ 0.00141178]\n",
      "Epoch: 4, Element: 0, Loss: [ 0.00338545]\n",
      "Epoch: 4, Element: 1, Loss: [ 0.00440103]\n",
      "Epoch: 4, Element: 2, Loss: [ 0.0028905]\n",
      "Epoch: 4, Element: 3, Loss: [ 0.00140711]\n",
      "Epoch: 5, Element: 0, Loss: [ 0.00337441]\n",
      "Epoch: 5, Element: 1, Loss: [ 0.00438652]\n",
      "Epoch: 5, Element: 2, Loss: [ 0.00288101]\n",
      "Epoch: 5, Element: 3, Loss: [ 0.00140247]\n",
      "Epoch: 6, Element: 0, Loss: [ 0.00336345]\n",
      "Epoch: 6, Element: 1, Loss: [ 0.0043721]\n",
      "Epoch: 6, Element: 2, Loss: [ 0.00287159]\n",
      "Epoch: 6, Element: 3, Loss: [ 0.00139786]\n",
      "Epoch: 7, Element: 0, Loss: [ 0.00335255]\n",
      "Epoch: 7, Element: 1, Loss: [ 0.00435778]\n",
      "Epoch: 7, Element: 2, Loss: [ 0.00286223]\n",
      "Epoch: 7, Element: 3, Loss: [ 0.00139328]\n",
      "Epoch: 8, Element: 0, Loss: [ 0.00334173]\n",
      "Epoch: 8, Element: 1, Loss: [ 0.00434355]\n",
      "Epoch: 8, Element: 2, Loss: [ 0.00285293]\n",
      "Epoch: 8, Element: 3, Loss: [ 0.00138873]\n",
      "Epoch: 9, Element: 0, Loss: [ 0.00333097]\n",
      "Epoch: 9, Element: 1, Loss: [ 0.00432941]\n",
      "Epoch: 9, Element: 2, Loss: [ 0.00284369]\n",
      "Epoch: 9, Element: 3, Loss: [ 0.0013842]\n",
      "Epoch: 10, Element: 0, Loss: [ 0.00332029]\n",
      "Epoch: 10, Element: 1, Loss: [ 0.00431537]\n",
      "Epoch: 10, Element: 2, Loss: [ 0.0028345]\n",
      "Epoch: 10, Element: 3, Loss: [ 0.00137971]\n",
      "Epoch: 11, Element: 0, Loss: [ 0.00330967]\n",
      "Epoch: 11, Element: 1, Loss: [ 0.00430142]\n",
      "Epoch: 11, Element: 2, Loss: [ 0.00282538]\n",
      "Epoch: 11, Element: 3, Loss: [ 0.00137525]\n",
      "Epoch: 12, Element: 0, Loss: [ 0.00329912]\n",
      "Epoch: 12, Element: 1, Loss: [ 0.00428755]\n",
      "Epoch: 12, Element: 2, Loss: [ 0.00281632]\n",
      "Epoch: 12, Element: 3, Loss: [ 0.00137081]\n",
      "Epoch: 13, Element: 0, Loss: [ 0.00328863]\n",
      "Epoch: 13, Element: 1, Loss: [ 0.00427378]\n",
      "Epoch: 13, Element: 2, Loss: [ 0.00280731]\n",
      "Epoch: 13, Element: 3, Loss: [ 0.00136641]\n",
      "Epoch: 14, Element: 0, Loss: [ 0.00327821]\n",
      "Epoch: 14, Element: 1, Loss: [ 0.00426009]\n",
      "Epoch: 14, Element: 2, Loss: [ 0.00279836]\n",
      "Epoch: 14, Element: 3, Loss: [ 0.00136203]\n",
      "Epoch: 15, Element: 0, Loss: [ 0.00326786]\n",
      "Epoch: 15, Element: 1, Loss: [ 0.0042465]\n",
      "Epoch: 15, Element: 2, Loss: [ 0.00278947]\n",
      "Epoch: 15, Element: 3, Loss: [ 0.00135768]\n",
      "Epoch: 16, Element: 0, Loss: [ 0.00325757]\n",
      "Epoch: 16, Element: 1, Loss: [ 0.00423299]\n",
      "Epoch: 16, Element: 2, Loss: [ 0.00278063]\n",
      "Epoch: 16, Element: 3, Loss: [ 0.00135336]\n",
      "Epoch: 17, Element: 0, Loss: [ 0.00324735]\n",
      "Epoch: 17, Element: 1, Loss: [ 0.00421956]\n",
      "Epoch: 17, Element: 2, Loss: [ 0.00277185]\n",
      "Epoch: 17, Element: 3, Loss: [ 0.00134907]\n",
      "Epoch: 18, Element: 0, Loss: [ 0.00323719]\n",
      "Epoch: 18, Element: 1, Loss: [ 0.00420622]\n",
      "Epoch: 18, Element: 2, Loss: [ 0.00276313]\n",
      "Epoch: 18, Element: 3, Loss: [ 0.0013448]\n",
      "Epoch: 19, Element: 0, Loss: [ 0.00322709]\n",
      "Epoch: 19, Element: 1, Loss: [ 0.00419296]\n",
      "Epoch: 19, Element: 2, Loss: [ 0.00275446]\n",
      "Epoch: 19, Element: 3, Loss: [ 0.00134056]\n",
      "Epoch: 20, Element: 0, Loss: [ 0.00321706]\n",
      "Epoch: 20, Element: 1, Loss: [ 0.00417979]\n",
      "Epoch: 20, Element: 2, Loss: [ 0.00274584]\n",
      "Epoch: 20, Element: 3, Loss: [ 0.00133635]\n",
      "Epoch: 21, Element: 0, Loss: [ 0.00320709]\n",
      "Epoch: 21, Element: 1, Loss: [ 0.0041667]\n",
      "Epoch: 21, Element: 2, Loss: [ 0.00273728]\n",
      "Epoch: 21, Element: 3, Loss: [ 0.00133216]\n",
      "Epoch: 22, Element: 0, Loss: [ 0.00319717]\n",
      "Epoch: 22, Element: 1, Loss: [ 0.00415369]\n",
      "Epoch: 22, Element: 2, Loss: [ 0.00272877]\n",
      "Epoch: 22, Element: 3, Loss: [ 0.001328]\n",
      "Epoch: 23, Element: 0, Loss: [ 0.00318733]\n",
      "Epoch: 23, Element: 1, Loss: [ 0.00414077]\n",
      "Epoch: 23, Element: 2, Loss: [ 0.00272031]\n",
      "Epoch: 23, Element: 3, Loss: [ 0.00132387]\n",
      "Epoch: 24, Element: 0, Loss: [ 0.00317754]\n",
      "Epoch: 24, Element: 1, Loss: [ 0.00412792]\n",
      "Epoch: 24, Element: 2, Loss: [ 0.0027119]\n",
      "Epoch: 24, Element: 3, Loss: [ 0.00131976]\n",
      "Epoch: 25, Element: 0, Loss: [ 0.00316781]\n",
      "Epoch: 25, Element: 1, Loss: [ 0.00411515]\n",
      "Epoch: 25, Element: 2, Loss: [ 0.00270355]\n",
      "Epoch: 25, Element: 3, Loss: [ 0.00131568]\n",
      "Epoch: 26, Element: 0, Loss: [ 0.00315814]\n",
      "Epoch: 26, Element: 1, Loss: [ 0.00410246]\n",
      "Epoch: 26, Element: 2, Loss: [ 0.00269525]\n",
      "Epoch: 26, Element: 3, Loss: [ 0.00131162]\n",
      "Epoch: 27, Element: 0, Loss: [ 0.00314853]\n",
      "Epoch: 27, Element: 1, Loss: [ 0.00408986]\n",
      "Epoch: 27, Element: 2, Loss: [ 0.002687]\n",
      "Epoch: 27, Element: 3, Loss: [ 0.00130759]\n",
      "Epoch: 28, Element: 0, Loss: [ 0.00313897]\n",
      "Epoch: 28, Element: 1, Loss: [ 0.00407732]\n",
      "Epoch: 28, Element: 2, Loss: [ 0.00267879]\n",
      "Epoch: 28, Element: 3, Loss: [ 0.00130358]\n",
      "Epoch: 29, Element: 0, Loss: [ 0.00312948]\n",
      "Epoch: 29, Element: 1, Loss: [ 0.00406486]\n",
      "Epoch: 29, Element: 2, Loss: [ 0.00267064]\n",
      "Epoch: 29, Element: 3, Loss: [ 0.00129959]\n",
      "Epoch: 30, Element: 0, Loss: [ 0.00312004]\n",
      "Epoch: 30, Element: 1, Loss: [ 0.00405249]\n",
      "Epoch: 30, Element: 2, Loss: [ 0.00266254]\n",
      "Epoch: 30, Element: 3, Loss: [ 0.00129563]\n",
      "Epoch: 31, Element: 0, Loss: [ 0.00311065]\n",
      "Epoch: 31, Element: 1, Loss: [ 0.00404018]\n",
      "Epoch: 31, Element: 2, Loss: [ 0.00265449]\n",
      "Epoch: 31, Element: 3, Loss: [ 0.0012917]\n",
      "Epoch: 32, Element: 0, Loss: [ 0.00310133]\n",
      "Epoch: 32, Element: 1, Loss: [ 0.00402795]\n",
      "Epoch: 32, Element: 2, Loss: [ 0.00264648]\n",
      "Epoch: 32, Element: 3, Loss: [ 0.00128779]\n",
      "Epoch: 33, Element: 0, Loss: [ 0.00309206]\n",
      "Epoch: 33, Element: 1, Loss: [ 0.0040158]\n",
      "Epoch: 33, Element: 2, Loss: [ 0.00263852]\n",
      "Epoch: 33, Element: 3, Loss: [ 0.0012839]\n",
      "Epoch: 34, Element: 0, Loss: [ 0.00308284]\n",
      "Epoch: 34, Element: 1, Loss: [ 0.00400371]\n",
      "Epoch: 34, Element: 2, Loss: [ 0.00263061]\n",
      "Epoch: 34, Element: 3, Loss: [ 0.00128004]\n",
      "Epoch: 35, Element: 0, Loss: [ 0.00307368]\n",
      "Epoch: 35, Element: 1, Loss: [ 0.0039917]\n",
      "Epoch: 35, Element: 2, Loss: [ 0.00262275]\n",
      "Epoch: 35, Element: 3, Loss: [ 0.0012762]\n",
      "Epoch: 36, Element: 0, Loss: [ 0.00306457]\n",
      "Epoch: 36, Element: 1, Loss: [ 0.00397977]\n",
      "Epoch: 36, Element: 2, Loss: [ 0.00261494]\n",
      "Epoch: 36, Element: 3, Loss: [ 0.00127238]\n",
      "Epoch: 37, Element: 0, Loss: [ 0.00305552]\n",
      "Epoch: 37, Element: 1, Loss: [ 0.0039679]\n",
      "Epoch: 37, Element: 2, Loss: [ 0.00260717]\n",
      "Epoch: 37, Element: 3, Loss: [ 0.00126858]\n",
      "Epoch: 38, Element: 0, Loss: [ 0.00304652]\n",
      "Epoch: 38, Element: 1, Loss: [ 0.0039561]\n",
      "Epoch: 38, Element: 2, Loss: [ 0.00259945]\n",
      "Epoch: 38, Element: 3, Loss: [ 0.00126481]\n",
      "Epoch: 39, Element: 0, Loss: [ 0.00303757]\n",
      "Epoch: 39, Element: 1, Loss: [ 0.00394438]\n",
      "Epoch: 39, Element: 2, Loss: [ 0.00259177]\n",
      "Epoch: 39, Element: 3, Loss: [ 0.00126106]\n",
      "Epoch: 40, Element: 0, Loss: [ 0.00302868]\n",
      "Epoch: 40, Element: 1, Loss: [ 0.00393272]\n",
      "Epoch: 40, Element: 2, Loss: [ 0.00258413]\n",
      "Epoch: 40, Element: 3, Loss: [ 0.00125733]\n",
      "Epoch: 41, Element: 0, Loss: [ 0.00301983]\n",
      "Epoch: 41, Element: 1, Loss: [ 0.00392113]\n",
      "Epoch: 41, Element: 2, Loss: [ 0.00257655]\n",
      "Epoch: 41, Element: 3, Loss: [ 0.00125363]\n",
      "Epoch: 42, Element: 0, Loss: [ 0.00301104]\n",
      "Epoch: 42, Element: 1, Loss: [ 0.00390961]\n",
      "Epoch: 42, Element: 2, Loss: [ 0.002569]\n",
      "Epoch: 42, Element: 3, Loss: [ 0.00124994]\n",
      "Epoch: 43, Element: 0, Loss: [ 0.0030023]\n",
      "Epoch: 43, Element: 1, Loss: [ 0.00389816]\n",
      "Epoch: 43, Element: 2, Loss: [ 0.0025615]\n",
      "Epoch: 43, Element: 3, Loss: [ 0.00124628]\n",
      "Epoch: 44, Element: 0, Loss: [ 0.00299361]\n",
      "Epoch: 44, Element: 1, Loss: [ 0.00388677]\n",
      "Epoch: 44, Element: 2, Loss: [ 0.00255405]\n",
      "Epoch: 44, Element: 3, Loss: [ 0.00124264]\n",
      "Epoch: 45, Element: 0, Loss: [ 0.00298497]\n",
      "Epoch: 45, Element: 1, Loss: [ 0.00387546]\n",
      "Epoch: 45, Element: 2, Loss: [ 0.00254663]\n",
      "Epoch: 45, Element: 3, Loss: [ 0.00123902]\n",
      "Epoch: 46, Element: 0, Loss: [ 0.00297638]\n",
      "Epoch: 46, Element: 1, Loss: [ 0.0038642]\n",
      "Epoch: 46, Element: 2, Loss: [ 0.00253927]\n",
      "Epoch: 46, Element: 3, Loss: [ 0.00123542]\n",
      "Epoch: 47, Element: 0, Loss: [ 0.00296784]\n",
      "Epoch: 47, Element: 1, Loss: [ 0.00385302]\n",
      "Epoch: 47, Element: 2, Loss: [ 0.00253194]\n",
      "Epoch: 47, Element: 3, Loss: [ 0.00123184]\n",
      "Epoch: 48, Element: 0, Loss: [ 0.00295934]\n",
      "Epoch: 48, Element: 1, Loss: [ 0.00384189]\n",
      "Epoch: 48, Element: 2, Loss: [ 0.00252465]\n",
      "Epoch: 48, Element: 3, Loss: [ 0.00122829]\n",
      "Epoch: 49, Element: 0, Loss: [ 0.0029509]\n",
      "Epoch: 49, Element: 1, Loss: [ 0.00383083]\n",
      "Epoch: 49, Element: 2, Loss: [ 0.00251741]\n",
      "Epoch: 49, Element: 3, Loss: [ 0.00122475]\n",
      "Epoch: 50, Element: 0, Loss: [ 0.0029425]\n",
      "Epoch: 50, Element: 1, Loss: [ 0.00381984]\n",
      "Epoch: 50, Element: 2, Loss: [ 0.00251021]\n",
      "Epoch: 50, Element: 3, Loss: [ 0.00122124]\n",
      "Epoch: 51, Element: 0, Loss: [ 0.00293415]\n",
      "Epoch: 51, Element: 1, Loss: [ 0.00380891]\n",
      "Epoch: 51, Element: 2, Loss: [ 0.00250304]\n",
      "Epoch: 51, Element: 3, Loss: [ 0.00121774]\n",
      "Epoch: 52, Element: 0, Loss: [ 0.00292585]\n",
      "Epoch: 52, Element: 1, Loss: [ 0.00379804]\n",
      "Epoch: 52, Element: 2, Loss: [ 0.00249592]\n",
      "Epoch: 52, Element: 3, Loss: [ 0.00121426]\n",
      "Epoch: 53, Element: 0, Loss: [ 0.0029176]\n",
      "Epoch: 53, Element: 1, Loss: [ 0.00378723]\n",
      "Epoch: 53, Element: 2, Loss: [ 0.00248885]\n",
      "Epoch: 53, Element: 3, Loss: [ 0.00121081]\n",
      "Epoch: 54, Element: 0, Loss: [ 0.00290939]\n",
      "Epoch: 54, Element: 1, Loss: [ 0.00377648]\n",
      "Epoch: 54, Element: 2, Loss: [ 0.0024818]\n",
      "Epoch: 54, Element: 3, Loss: [ 0.00120737]\n",
      "Epoch: 55, Element: 0, Loss: [ 0.00290122]\n",
      "Epoch: 55, Element: 1, Loss: [ 0.0037658]\n",
      "Epoch: 55, Element: 2, Loss: [ 0.0024748]\n",
      "Epoch: 55, Element: 3, Loss: [ 0.00120395]\n",
      "Epoch: 56, Element: 0, Loss: [ 0.00289311]\n",
      "Epoch: 56, Element: 1, Loss: [ 0.00375517]\n",
      "Epoch: 56, Element: 2, Loss: [ 0.00246784]\n",
      "Epoch: 56, Element: 3, Loss: [ 0.00120056]\n",
      "Epoch: 57, Element: 0, Loss: [ 0.00288503]\n",
      "Epoch: 57, Element: 1, Loss: [ 0.00374461]\n",
      "Epoch: 57, Element: 2, Loss: [ 0.00246092]\n",
      "Epoch: 57, Element: 3, Loss: [ 0.00119718]\n",
      "Epoch: 58, Element: 0, Loss: [ 0.00287701]\n",
      "Epoch: 58, Element: 1, Loss: [ 0.0037341]\n",
      "Epoch: 58, Element: 2, Loss: [ 0.00245404]\n",
      "Epoch: 58, Element: 3, Loss: [ 0.00119382]\n",
      "Epoch: 59, Element: 0, Loss: [ 0.00286902]\n",
      "Epoch: 59, Element: 1, Loss: [ 0.00372365]\n",
      "Epoch: 59, Element: 2, Loss: [ 0.00244719]\n",
      "Epoch: 59, Element: 3, Loss: [ 0.00119048]\n",
      "Epoch: 60, Element: 0, Loss: [ 0.00286108]\n",
      "Epoch: 60, Element: 1, Loss: [ 0.00371327]\n",
      "Epoch: 60, Element: 2, Loss: [ 0.00244038]\n",
      "Epoch: 60, Element: 3, Loss: [ 0.00118716]\n",
      "Epoch: 61, Element: 0, Loss: [ 0.00285319]\n",
      "Epoch: 61, Element: 1, Loss: [ 0.00370294]\n",
      "Epoch: 61, Element: 2, Loss: [ 0.00243361]\n",
      "Epoch: 61, Element: 3, Loss: [ 0.00118385]\n",
      "Epoch: 62, Element: 0, Loss: [ 0.00284533]\n",
      "Epoch: 62, Element: 1, Loss: [ 0.00369266]\n",
      "Epoch: 62, Element: 2, Loss: [ 0.00242688]\n",
      "Epoch: 62, Element: 3, Loss: [ 0.00118057]\n",
      "Epoch: 63, Element: 0, Loss: [ 0.00283752]\n",
      "Epoch: 63, Element: 1, Loss: [ 0.00368244]\n",
      "Epoch: 63, Element: 2, Loss: [ 0.00242019]\n",
      "Epoch: 63, Element: 3, Loss: [ 0.0011773]\n",
      "Epoch: 64, Element: 0, Loss: [ 0.00282976]\n",
      "Epoch: 64, Element: 1, Loss: [ 0.00367229]\n",
      "Epoch: 64, Element: 2, Loss: [ 0.00241353]\n",
      "Epoch: 64, Element: 3, Loss: [ 0.00117405]\n",
      "Epoch: 65, Element: 0, Loss: [ 0.00282203]\n",
      "Epoch: 65, Element: 1, Loss: [ 0.00366218]\n",
      "Epoch: 65, Element: 2, Loss: [ 0.00240691]\n",
      "Epoch: 65, Element: 3, Loss: [ 0.00117082]\n",
      "Epoch: 66, Element: 0, Loss: [ 0.00281435]\n",
      "Epoch: 66, Element: 1, Loss: [ 0.00365213]\n",
      "Epoch: 66, Element: 2, Loss: [ 0.00240032]\n",
      "Epoch: 66, Element: 3, Loss: [ 0.00116761]\n",
      "Epoch: 67, Element: 0, Loss: [ 0.00280671]\n",
      "Epoch: 67, Element: 1, Loss: [ 0.00364214]\n",
      "Epoch: 67, Element: 2, Loss: [ 0.00239377]\n",
      "Epoch: 67, Element: 3, Loss: [ 0.00116441]\n",
      "Epoch: 68, Element: 0, Loss: [ 0.00279911]\n",
      "Epoch: 68, Element: 1, Loss: [ 0.0036322]\n",
      "Epoch: 68, Element: 2, Loss: [ 0.00238726]\n",
      "Epoch: 68, Element: 3, Loss: [ 0.00116124]\n",
      "Epoch: 69, Element: 0, Loss: [ 0.00279155]\n",
      "Epoch: 69, Element: 1, Loss: [ 0.00362232]\n",
      "Epoch: 69, Element: 2, Loss: [ 0.00238078]\n",
      "Epoch: 69, Element: 3, Loss: [ 0.00115808]\n",
      "Epoch: 70, Element: 0, Loss: [ 0.00278403]\n",
      "Epoch: 70, Element: 1, Loss: [ 0.00361249]\n",
      "Epoch: 70, Element: 2, Loss: [ 0.00237433]\n",
      "Epoch: 70, Element: 3, Loss: [ 0.00115493]\n",
      "Epoch: 71, Element: 0, Loss: [ 0.00277656]\n",
      "Epoch: 71, Element: 1, Loss: [ 0.00360271]\n",
      "Epoch: 71, Element: 2, Loss: [ 0.00236792]\n",
      "Epoch: 71, Element: 3, Loss: [ 0.00115181]\n",
      "Epoch: 72, Element: 0, Loss: [ 0.00276912]\n",
      "Epoch: 72, Element: 1, Loss: [ 0.00359298]\n",
      "Epoch: 72, Element: 2, Loss: [ 0.00236155]\n",
      "Epoch: 72, Element: 3, Loss: [ 0.0011487]\n",
      "Epoch: 73, Element: 0, Loss: [ 0.00276172]\n",
      "Epoch: 73, Element: 1, Loss: [ 0.00358331]\n",
      "Epoch: 73, Element: 2, Loss: [ 0.00235521]\n",
      "Epoch: 73, Element: 3, Loss: [ 0.0011456]\n",
      "Epoch: 74, Element: 0, Loss: [ 0.00275436]\n",
      "Epoch: 74, Element: 1, Loss: [ 0.00357369]\n",
      "Epoch: 74, Element: 2, Loss: [ 0.0023489]\n",
      "Epoch: 74, Element: 3, Loss: [ 0.00114253]\n",
      "Epoch: 75, Element: 0, Loss: [ 0.00274704]\n",
      "Epoch: 75, Element: 1, Loss: [ 0.00356412]\n",
      "Epoch: 75, Element: 2, Loss: [ 0.00234263]\n",
      "Epoch: 75, Element: 3, Loss: [ 0.00113947]\n",
      "Epoch: 76, Element: 0, Loss: [ 0.00273976]\n",
      "Epoch: 76, Element: 1, Loss: [ 0.00355461]\n",
      "Epoch: 76, Element: 2, Loss: [ 0.00233639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 76, Element: 3, Loss: [ 0.00113643]\n",
      "Epoch: 77, Element: 0, Loss: [ 0.00273252]\n",
      "Epoch: 77, Element: 1, Loss: [ 0.00354514]\n",
      "Epoch: 77, Element: 2, Loss: [ 0.00233018]\n",
      "Epoch: 77, Element: 3, Loss: [ 0.0011334]\n",
      "Epoch: 78, Element: 0, Loss: [ 0.00272531]\n",
      "Epoch: 78, Element: 1, Loss: [ 0.00353572]\n",
      "Epoch: 78, Element: 2, Loss: [ 0.00232401]\n",
      "Epoch: 78, Element: 3, Loss: [ 0.00113039]\n",
      "Epoch: 79, Element: 0, Loss: [ 0.00271815]\n",
      "Epoch: 79, Element: 1, Loss: [ 0.00352636]\n",
      "Epoch: 79, Element: 2, Loss: [ 0.00231787]\n",
      "Epoch: 79, Element: 3, Loss: [ 0.00112739]\n",
      "Epoch: 80, Element: 0, Loss: [ 0.00271102]\n",
      "Epoch: 80, Element: 1, Loss: [ 0.00351704]\n",
      "Epoch: 80, Element: 2, Loss: [ 0.00231176]\n",
      "Epoch: 80, Element: 3, Loss: [ 0.00112441]\n",
      "Epoch: 81, Element: 0, Loss: [ 0.00270393]\n",
      "Epoch: 81, Element: 1, Loss: [ 0.00350777]\n",
      "Epoch: 81, Element: 2, Loss: [ 0.00230568]\n",
      "Epoch: 81, Element: 3, Loss: [ 0.00112145]\n",
      "Epoch: 82, Element: 0, Loss: [ 0.00269687]\n",
      "Epoch: 82, Element: 1, Loss: [ 0.00349855]\n",
      "Epoch: 82, Element: 2, Loss: [ 0.00229963]\n",
      "Epoch: 82, Element: 3, Loss: [ 0.0011185]\n",
      "Epoch: 83, Element: 0, Loss: [ 0.00268985]\n",
      "Epoch: 83, Element: 1, Loss: [ 0.00348938]\n",
      "Epoch: 83, Element: 2, Loss: [ 0.00229362]\n",
      "Epoch: 83, Element: 3, Loss: [ 0.00111557]\n",
      "Epoch: 84, Element: 0, Loss: [ 0.00268287]\n",
      "Epoch: 84, Element: 1, Loss: [ 0.00348026]\n",
      "Epoch: 84, Element: 2, Loss: [ 0.00228764]\n",
      "Epoch: 84, Element: 3, Loss: [ 0.00111265]\n",
      "Epoch: 85, Element: 0, Loss: [ 0.00267592]\n",
      "Epoch: 85, Element: 1, Loss: [ 0.00347119]\n",
      "Epoch: 85, Element: 2, Loss: [ 0.00228169]\n",
      "Epoch: 85, Element: 3, Loss: [ 0.00110975]\n",
      "Epoch: 86, Element: 0, Loss: [ 0.00266901]\n",
      "Epoch: 86, Element: 1, Loss: [ 0.00346216]\n",
      "Epoch: 86, Element: 2, Loss: [ 0.00227577]\n",
      "Epoch: 86, Element: 3, Loss: [ 0.00110687]\n",
      "Epoch: 87, Element: 0, Loss: [ 0.00266214]\n",
      "Epoch: 87, Element: 1, Loss: [ 0.00345318]\n",
      "Epoch: 87, Element: 2, Loss: [ 0.00226988]\n",
      "Epoch: 87, Element: 3, Loss: [ 0.00110399]\n",
      "Epoch: 88, Element: 0, Loss: [ 0.0026553]\n",
      "Epoch: 88, Element: 1, Loss: [ 0.00344424]\n",
      "Epoch: 88, Element: 2, Loss: [ 0.00226402]\n",
      "Epoch: 88, Element: 3, Loss: [ 0.00110114]\n",
      "Epoch: 89, Element: 0, Loss: [ 0.00264849]\n",
      "Epoch: 89, Element: 1, Loss: [ 0.00343535]\n",
      "Epoch: 89, Element: 2, Loss: [ 0.00225819]\n",
      "Epoch: 89, Element: 3, Loss: [ 0.0010983]\n",
      "Epoch: 90, Element: 0, Loss: [ 0.00264172]\n",
      "Epoch: 90, Element: 1, Loss: [ 0.00342651]\n",
      "Epoch: 90, Element: 2, Loss: [ 0.00225239]\n",
      "Epoch: 90, Element: 3, Loss: [ 0.00109547]\n",
      "Epoch: 91, Element: 0, Loss: [ 0.00263499]\n",
      "Epoch: 91, Element: 1, Loss: [ 0.00341772]\n",
      "Epoch: 91, Element: 2, Loss: [ 0.00224662]\n",
      "Epoch: 91, Element: 3, Loss: [ 0.00109266]\n",
      "Epoch: 92, Element: 0, Loss: [ 0.00262829]\n",
      "Epoch: 92, Element: 1, Loss: [ 0.00340896]\n",
      "Epoch: 92, Element: 2, Loss: [ 0.00224088]\n",
      "Epoch: 92, Element: 3, Loss: [ 0.00108986]\n",
      "Epoch: 93, Element: 0, Loss: [ 0.00262162]\n",
      "Epoch: 93, Element: 1, Loss: [ 0.00340025]\n",
      "Epoch: 93, Element: 2, Loss: [ 0.00223517]\n",
      "Epoch: 93, Element: 3, Loss: [ 0.00108707]\n",
      "Epoch: 94, Element: 0, Loss: [ 0.00261498]\n",
      "Epoch: 94, Element: 1, Loss: [ 0.00339159]\n",
      "Epoch: 94, Element: 2, Loss: [ 0.00222948]\n",
      "Epoch: 94, Element: 3, Loss: [ 0.0010843]\n",
      "Epoch: 95, Element: 0, Loss: [ 0.00260839]\n",
      "Epoch: 95, Element: 1, Loss: [ 0.00338298]\n",
      "Epoch: 95, Element: 2, Loss: [ 0.00222383]\n",
      "Epoch: 95, Element: 3, Loss: [ 0.00108155]\n",
      "Epoch: 96, Element: 0, Loss: [ 0.00260182]\n",
      "Epoch: 96, Element: 1, Loss: [ 0.0033744]\n",
      "Epoch: 96, Element: 2, Loss: [ 0.00221821]\n",
      "Epoch: 96, Element: 3, Loss: [ 0.00107881]\n",
      "Epoch: 97, Element: 0, Loss: [ 0.00259529]\n",
      "Epoch: 97, Element: 1, Loss: [ 0.00336587]\n",
      "Epoch: 97, Element: 2, Loss: [ 0.00221261]\n",
      "Epoch: 97, Element: 3, Loss: [ 0.00107608]\n",
      "Epoch: 98, Element: 0, Loss: [ 0.00258878]\n",
      "Epoch: 98, Element: 1, Loss: [ 0.00335738]\n",
      "Epoch: 98, Element: 2, Loss: [ 0.00220704]\n",
      "Epoch: 98, Element: 3, Loss: [ 0.00107337]\n",
      "Epoch: 99, Element: 0, Loss: [ 0.00258231]\n",
      "Epoch: 99, Element: 1, Loss: [ 0.00334893]\n",
      "Epoch: 99, Element: 2, Loss: [ 0.0022015]\n",
      "Epoch: 99, Element: 3, Loss: [ 0.00107067]\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "for epoch in range(100):\n",
    "    counter = 0\n",
    "    for instance, label in data:\n",
    "        model.zero_grad()\n",
    "        \n",
    "        bow_vec = autograd.Variable(make_bow_vector(instance, word_to_idx))\n",
    "        target = autograd.Variable(make_target(label, label_to_idx))\n",
    "        \n",
    "        log_probs = model(bow_vec)\n",
    "        \n",
    "        loss = loss_function(log_probs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(\"Epoch: {}, Element: {}, Loss: {}\".format(epoch, counter, loss.data.numpy()))\n",
    "        counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0994 -2.3576\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "Variable containing:\n",
      "-2.9547 -0.0535\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on the test data\n",
    "for instance, label in test_data:\n",
    "    bow_vec = autograd.Variable(make_bow_vector(instance, word_to_idx))\n",
    "    log_probs = model(bow_vec)\n",
    "    print(log_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.6395\n",
      "-0.7026\n",
      "[torch.FloatTensor of size 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print parameters corresponging to the word \"creo\"\n",
    "print(next(model.parameters())[:, word_to_idx[\"creo\"]])\n",
    "\n",
    "# We can see that the element 0 (corresponding to Spanish has gone up while the other has gone down)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.1326 -0.0228  1.1848 -1.0322 -0.7039\n",
      "[torch.FloatTensor of size 1x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Small example\n",
    "word_to_idx = {\"hello\": 0, \"world\": 1}\n",
    "embeds = nn.Embedding(2, 5) # 2 is the vocabulary size, 5-dimensional embeddings\n",
    "lookup_tensor = torch.LongTensor([word_to_idx[\"hello\"]])\n",
    "hello_embed = embeds(autograd.Variable(lookup_tensor))\n",
    "print(hello_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['When', 'forty'], 'winters'), (['forty', 'winters'], 'shall'), (['winters', 'shall'], 'besiege')]\n"
     ]
    }
   ],
   "source": [
    "CONTEXT_SIZE = 2\n",
    "EMBEDDING_DIM = 10\n",
    "\n",
    "test_sentence = \"\"\"When forty winters shall besiege thy brow,\n",
    "And dig deep trenches in thy beauty's field,\n",
    "Thy youth's proud livery so gazed on now,\n",
    "Will be a totter'd weed of small worth held:\n",
    "Then being asked, where all thy beauty lies,\n",
    "Where all the treasure of thy lusty days;\n",
    "To say, within thine own deep sunken eyes,\n",
    "Were an all-eating shame, and thriftless praise.\n",
    "How much more praise deserv'd thy beauty's use,\n",
    "If thou couldst answer 'This fair child of mine\n",
    "Shall sum my count, and make my old excuse,'\n",
    "Proving his beauty by succession thine!\n",
    "This were to be new made when thou art old,\n",
    "And see thy blood warm when thou feel'st it cold.\"\"\".split()\n",
    "\n",
    "trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence) - 2)]\n",
    "print(trigrams[:3])\n",
    "\n",
    "vocab = set(test_sentence)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NGramLanguageModeler(nn.Module):\n",
    "    '''\n",
    "        The N-Gram Language Model\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, context_size):\n",
    "        super(NGramLanguageModeler, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.linear1 = nn.Linear(context_size * embedding_dim, 128)\n",
    "        self.linear2 = nn.Linear(128, vocab_size)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        embeds = self.embeddings(inputs).view((1,-1))\n",
    "        out = F.relu(self.linear1(embeds))\n",
    "        out = self.linear2(out)\n",
    "        log_probs = F.log_softmax(out)\n",
    "        return log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " 524.1758\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 521.5498\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 518.9454\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 516.3609\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 513.7955\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 511.2482\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 508.7176\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 506.2034\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 503.7036\n",
      "[torch.FloatTensor of size 1]\n",
      ", \n",
      " 501.2171\n",
      "[torch.FloatTensor of size 1]\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# Train\n",
    "for epoch in range(10):\n",
    "    \n",
    "    total_loss = torch.Tensor([0])\n",
    "    \n",
    "    for context, target in trigrams:\n",
    "        \n",
    "        context_idxs = [word_to_idx[w] for w in context]\n",
    "        context_var = autograd.Variable(torch.LongTensor(context_idxs))\n",
    "        \n",
    "        model.zero_grad()\n",
    "        \n",
    "        log_probs = model(context_var)\n",
    "        \n",
    "        loss = loss_function(log_probs, autograd.Variable(torch.LongTensor([word_to_idx[target]])))\n",
    "\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.data\n",
    "        \n",
    "    losses.append(total_loss)\n",
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
